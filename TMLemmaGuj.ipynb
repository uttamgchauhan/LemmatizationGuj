{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling for TDIL dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Documents: 7786\n",
      "[['ગ્રેટર', 'નોઈડા', 'વેસ્ટની', 'મુશ્કેલીઓ', 'નામ', 'માંગણીઓને', 'નિર્માણકાર્ય', 'બંધ', 'ખેડૂત', 'સંગઠનોમાં', 'હરીફાઈ', 'શરૂ', 'રવિવારે', 'માયચા', 'ગામમાં', 'ખેડૂત', 'સંઘર્ષ', 'સમિતિએ', 'પંચાયત', 'બોલાવીને', 'ઓક્ટોબરથી', 'નિર્માણકાર્ય', 'બંધ', 'નિર્ણય', 'વળતર', 'આપવાના', 'ખેડૂતોની', 'પ્રાધિકરણ', 'એડીએમ', 'એલ', 'કાર્યાલય', 'કરોડ', 'રૂપિયા', 'ગામડાઓમાં', 'મજાક', 'પંચાયતમાં', 'ખેડૂતોએ', 'પ્રાધિકરણ', 'ખેડૂતોની', 'સમસ્યાઓ', 'ગંભીર', 'શાસન', 'સપ્ટેમ્બરની', 'વાતચીતમાં', 'ખેડૂતોને', 'ઓક્ટોબર', 'ગામડાઓમાં', 'વળતર', 'આપવાનું', 'આશ્વાસન', 'પંચાયતમાં', 'ખેડૂતોએ', 'નિર્ણય', 'ઓક્ટોબરે', 'ત્રણ', 'ચાર', 'ટીમ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટમાં', 'નિર્માણ', 'કાર્ય', 'બંધ', 'શેરડી', 'મગફળી', 'બટાકા', 'વગેરે', 'પાક', 'આફત', 'આવનારી', 'સફેદ', 'ઈયળ', 'સફેદ', 'લટ', 'સફેદ', 'ગિલારના', 'નામથી', 'ઓળખવામાં', 'અસાધ્ય', 'ભારતીય', 'કૃષિ', 'અનુસંધાન', 'પરિષદના', 'આઈસીએઆર', 'રાષ્ટ્રીય', 'કૃષિ', 'ઉપયોગી', 'કીટ', 'બ્યૂરોના', 'વૈજ્ઞાનિકોએ', 'પદ્ધતિ', 'વિકસાવી', 'રોગનો', 'ટકા', 'ઈલાજ', 'સંભવ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટની', 'મુશ્કેલીઓ', 'નામ']]\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Tokenization\n",
    "# Read text file line by line\n",
    "# Tokenize each line using nltk word tokenizer\n",
    "# extend to the list i.e one list for whole document\n",
    "\n",
    "documents = []\n",
    "content = open(r'C:/Users/Chauhan/LemmaGuj/tdil_combined/preprocessed/all.txt',encoding='utf-8-sig').readlines()\n",
    "for line in content:\n",
    "    documents.append([word for word in nltk.word_tokenize(line)])\n",
    "    #documents.append(nltk.word_tokenize(line))\n",
    "print('Total Number of Documents:',len(documents))\n",
    "print(documents[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No. of stopwords 854\n",
      "Total Number of Documents after stopword removal: 7786\n",
      "[['ગ્રેટર', 'નોઈડા', 'વેસ્ટની', 'મુશ્કેલીઓ', 'નામ', 'માંગણીઓને', 'નિર્માણકાર્ય', 'બંધ', 'ખેડૂત', 'સંગઠનોમાં', 'હરીફાઈ', 'શરૂ', 'રવિવારે', 'માયચા', 'ગામમાં', 'ખેડૂત', 'સંઘર્ષ', 'સમિતિએ', 'પંચાયત', 'બોલાવીને', 'ઓક્ટોબરથી', 'નિર્માણકાર્ય', 'બંધ', 'નિર્ણય', 'વળતર', 'આપવાના', 'ખેડૂતોની', 'પ્રાધિકરણ', 'એડીએમ', 'એલ', 'કાર્યાલય', 'કરોડ', 'રૂપિયા', 'ગામડાઓમાં', 'મજાક', 'પંચાયતમાં', 'ખેડૂતોએ', 'પ્રાધિકરણ', 'ખેડૂતોની', 'સમસ્યાઓ', 'ગંભીર', 'શાસન', 'સપ્ટેમ્બરની', 'વાતચીતમાં', 'ખેડૂતોને', 'ઓક્ટોબર', 'ગામડાઓમાં', 'વળતર', 'આપવાનું', 'આશ્વાસન', 'પંચાયતમાં', 'ખેડૂતોએ', 'નિર્ણય', 'ઓક્ટોબરે', 'ત્રણ', 'ચાર', 'ટીમ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટમાં', 'નિર્માણ', 'કાર્ય', 'બંધ', 'શેરડી', 'મગફળી', 'બટાકા', 'વગેરે', 'પાક', 'આફત', 'આવનારી', 'સફેદ', 'ઈયળ', 'સફેદ', 'લટ', 'સફેદ', 'ગિલારના', 'નામથી', 'ઓળખવામાં', 'અસાધ્ય', 'ભારતીય', 'કૃષિ', 'અનુસંધાન', 'પરિષદના', 'આઈસીએઆર', 'રાષ્ટ્રીય', 'કૃષિ', 'ઉપયોગી', 'કીટ', 'બ્યૂરોના', 'વૈજ્ઞાનિકોએ', 'પદ્ધતિ', 'વિકસાવી', 'રોગનો', 'ટકા', 'ઈલાજ', 'સંભવ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટની', 'મુશ્કેલીઓ', 'નામ']]\n"
     ]
    }
   ],
   "source": [
    "# Step - 2 Stopwords Removal\n",
    "# Stopwords have been prepared for news articles - Gujarati\n",
    "\n",
    "\n",
    "# loading the stopwords in the list 'stopwords []' from the text file 'stop_words.txt'\n",
    "\n",
    "stopwords = nltk.word_tokenize(open(r'C:/Users/Chauhan/LemmaGuj/stop_words.txt',encoding='utf-8-sig').read())\n",
    "print('Total No. of stopwords',len(stopwords))\n",
    "# Remove the stopwords\n",
    "documents = [[word for word in document if word not in stopwords] for document in documents]\n",
    "print('Total Number of Documents after stopword removal:',len(documents))\n",
    "print(documents[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No. of tokens after punctuations removal: 7786\n",
      "[['ગ્રેટર', 'નોઈડા', 'વેસ્ટની', 'મુશ્કેલીઓ', 'નામ', 'માંગણીઓને', 'નિર્માણકાર્ય', 'બંધ', 'ખેડૂત', 'સંગઠનોમાં', 'હરીફાઈ', 'શરૂ', 'રવિવારે', 'માયચા', 'ગામમાં', 'ખેડૂત', 'સંઘર્ષ', 'સમિતિએ', 'પંચાયત', 'બોલાવીને', 'ઓક્ટોબરથી', 'નિર્માણકાર્ય', 'બંધ', 'નિર્ણય', 'વળતર', 'આપવાના', 'ખેડૂતોની', 'પ્રાધિકરણ', 'એડીએમ', 'એલ', 'કાર્યાલય', 'કરોડ', 'રૂપિયા', 'ગામડાઓમાં', 'મજાક', 'પંચાયતમાં', 'ખેડૂતોએ', 'પ્રાધિકરણ', 'ખેડૂતોની', 'સમસ્યાઓ', 'ગંભીર', 'શાસન', 'સપ્ટેમ્બરની', 'વાતચીતમાં', 'ખેડૂતોને', 'ઓક્ટોબર', 'ગામડાઓમાં', 'વળતર', 'આપવાનું', 'આશ્વાસન', 'પંચાયતમાં', 'ખેડૂતોએ', 'નિર્ણય', 'ઓક્ટોબરે', 'ત્રણ', 'ચાર', 'ટીમ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટમાં', 'નિર્માણ', 'કાર્ય', 'બંધ', 'શેરડી', 'મગફળી', 'બટાકા', 'વગેરે', 'પાક', 'આફત', 'આવનારી', 'સફેદ', 'ઈયળ', 'સફેદ', 'લટ', 'સફેદ', 'ગિલારના', 'નામથી', 'ઓળખવામાં', 'અસાધ્ય', 'ભારતીય', 'કૃષિ', 'અનુસંધાન', 'પરિષદના', 'આઈસીએઆર', 'રાષ્ટ્રીય', 'કૃષિ', 'ઉપયોગી', 'કીટ', 'બ્યૂરોના', 'વૈજ્ઞાનિકોએ', 'પદ્ધતિ', 'વિકસાવી', 'રોગનો', 'ટકા', 'ઈલાજ', 'સંભવ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટની', 'મુશ્કેલીઓ', 'નામ']]\n"
     ]
    }
   ],
   "source": [
    "# Step - 3 Remove punctuations symbols from the documents\n",
    "\n",
    "import string\n",
    "\n",
    "# Fetch all punctuation symbols in the list named 'punctuations []'\n",
    "\n",
    "punctuations = list(string.punctuation)\n",
    "documents = [[word for word in document if word not in punctuations] for document in documents]\n",
    "\n",
    "#print(punctuations)\n",
    "\n",
    "print('Total No. of tokens after punctuations removal:',len(documents))\n",
    "\n",
    "print(documents[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits_from_words(alphanumeric_words_list):\n",
    "    '''\n",
    "        list[] -> list[]\n",
    "        It removes the digits from words which are blended with digits\n",
    "        \n",
    "        Returns the alphabetic list\n",
    "    \n",
    "        >>> single_letter_word_removal(['૧ઉત્તમ૨૩','૧૨૩૪ચૌહાન','ઉ૨ત્તમ૧૨૩૪ચૌહાન']\n",
    "        >>> ['ઉત્તમ','ચૌહાન','ઉત્તમચૌહાન'])\n",
    "        \n",
    "    '''\n",
    "    alphabetic_list=[]\n",
    "    regex_removedigit = re.compile(r'\\d+', re.UNICODE)\n",
    "    for word in alphanumeric_words_list:\n",
    "        alphabetic_list.append(regex_removedigit.sub('',word))\n",
    "    return alphabetic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No. of tokens after single-letter words removal: 7786\n",
      "[['ગ્રેટર', 'નોઈડા', 'વેસ્ટની', 'મુશ્કેલીઓ', 'નામ', 'માંગણીઓને', 'નિર્માણકાર્ય', 'બંધ', 'ખેડૂત', 'સંગઠનોમાં', 'હરીફાઈ', 'શરૂ', 'રવિવારે', 'માયચા', 'ગામમાં', 'ખેડૂત', 'સંઘર્ષ', 'સમિતિએ', 'પંચાયત', 'બોલાવીને', 'ઓક્ટોબરથી', 'નિર્માણકાર્ય', 'બંધ', 'નિર્ણય', 'વળતર', 'આપવાના', 'ખેડૂતોની', 'પ્રાધિકરણ', 'એડીએમ', 'એલ', 'કાર્યાલય', 'કરોડ', 'રૂપિયા', 'ગામડાઓમાં', 'મજાક', 'પંચાયતમાં', 'ખેડૂતોએ', 'પ્રાધિકરણ', 'ખેડૂતોની', 'સમસ્યાઓ', 'ગંભીર', 'શાસન', 'સપ્ટેમ્બરની', 'વાતચીતમાં', 'ખેડૂતોને', 'ઓક્ટોબર', 'ગામડાઓમાં', 'વળતર', 'આપવાનું', 'આશ્વાસન', 'પંચાયતમાં', 'ખેડૂતોએ', 'નિર્ણય', 'ઓક્ટોબરે', 'ત્રણ', 'ચાર', 'ટીમ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટમાં', 'નિર્માણ', 'કાર્ય', 'બંધ', 'શેરડી', 'મગફળી', 'બટાકા', 'વગેરે', 'પાક', 'આફત', 'આવનારી', 'સફેદ', 'ઈયળ', 'સફેદ', 'લટ', 'સફેદ', 'ગિલારના', 'નામથી', 'ઓળખવામાં', 'અસાધ્ય', 'ભારતીય', 'કૃષિ', 'અનુસંધાન', 'પરિષદના', 'આઈસીએઆર', 'રાષ્ટ્રીય', 'કૃષિ', 'ઉપયોગી', 'કીટ', 'બ્યૂરોના', 'વૈજ્ઞાનિકોએ', 'પદ્ધતિ', 'વિકસાવી', 'રોગનો', 'ટકા', 'ઈલાજ', 'સંભવ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટની', 'મુશ્કેલીઓ', 'નામ']]\n"
     ]
    }
   ],
   "source": [
    "# Step - 4 Remove words blended with digits\n",
    "# ['૧ઉત્તમ૨૩','૧૨૩૪ચૌહાન','ઉ૨ત્તમ૧૨૩૪ચૌહાન'] ->  ['ઉત્તમ','ચૌહાન','ઉત્તમચૌહાન']\n",
    "import re\n",
    "dgr_documents=[]\n",
    "\n",
    "for document in documents:\n",
    "    dgr_documents.append(remove_digits_from_words(document))\n",
    "print('Total No. of tokens after single-letter words removal:',len(dgr_documents))\n",
    "print(dgr_documents[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition for the single-letter words removal\n",
    "# The Gujarati text contains remarkable quantity of sigle-letter words due to its morphological sturcutre\n",
    "import re\n",
    "def single_letter_word_removal(mixed_single_letter_words_list):\n",
    "    \n",
    "        '''\n",
    "        list[] -> list[]\n",
    "        It removes the single letter words from documents\n",
    "        1) It removes the suffixes from the end of the word\n",
    "        2) Checks the length of word if it is equal to 1\n",
    "        3) Removes the words from the list if length is 1.\n",
    "        4) Returns the purified list\n",
    "    \n",
    "        >>> single_letter_word_removal(['ક','કા','કમળના','કુ','કી'])\n",
    "            >>> ['કમળના']\n",
    "        '''\n",
    "        removed_single_letter_word_list = []\n",
    "        regex_singleletterremoval =  re.compile('(ી|ઁ|ુ|ં|ૂ|ઃ|ૅ|ે|ૈ|ૉ|ો|ૌ|્|઼|ા|િ|ૢ|ૣ|ૃ|ૄ|ી)',re.UNICODE)\n",
    "        for word in mixed_single_letter_words_list:\n",
    "            if (len(regex_singleletterremoval.sub(\"\",word))==1):\n",
    "                continue\n",
    "            else:\n",
    "                removed_single_letter_word_list.append(word)\n",
    "        return(removed_single_letter_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No. of tokens after single-letter words removal: 7786\n",
      "[['ગ્રેટર', 'નોઈડા', 'વેસ્ટની', 'મુશ્કેલીઓ', 'નામ', 'માંગણીઓને', 'નિર્માણકાર્ય', 'બંધ', 'ખેડૂત', 'સંગઠનોમાં', 'હરીફાઈ', 'શરૂ', 'રવિવારે', 'માયચા', 'ગામમાં', 'ખેડૂત', 'સંઘર્ષ', 'સમિતિએ', 'પંચાયત', 'બોલાવીને', 'ઓક્ટોબરથી', 'નિર્માણકાર્ય', 'બંધ', 'નિર્ણય', 'વળતર', 'આપવાના', 'ખેડૂતોની', 'પ્રાધિકરણ', 'એડીએમ', 'એલ', 'કાર્યાલય', 'કરોડ', 'રૂપિયા', 'ગામડાઓમાં', 'મજાક', 'પંચાયતમાં', 'ખેડૂતોએ', 'પ્રાધિકરણ', 'ખેડૂતોની', 'સમસ્યાઓ', 'ગંભીર', 'શાસન', 'સપ્ટેમ્બરની', 'વાતચીતમાં', 'ખેડૂતોને', 'ઓક્ટોબર', 'ગામડાઓમાં', 'વળતર', 'આપવાનું', 'આશ્વાસન', 'પંચાયતમાં', 'ખેડૂતોએ', 'નિર્ણય', 'ઓક્ટોબરે', 'ત્રણ', 'ચાર', 'ટીમ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટમાં', 'નિર્માણ', 'કાર્ય', 'બંધ', 'શેરડી', 'મગફળી', 'બટાકા', 'વગેરે', 'પાક', 'આફત', 'આવનારી', 'સફેદ', 'ઈયળ', 'સફેદ', 'લટ', 'સફેદ', 'ગિલારના', 'નામથી', 'ઓળખવામાં', 'અસાધ્ય', 'ભારતીય', 'કૃષિ', 'અનુસંધાન', 'પરિષદના', 'આઈસીએઆર', 'રાષ્ટ્રીય', 'કૃષિ', 'ઉપયોગી', 'કીટ', 'બ્યૂરોના', 'વૈજ્ઞાનિકોએ', 'પદ્ધતિ', 'વિકસાવી', 'રોગનો', 'ટકા', 'ઈલાજ', 'સંભવ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટની', 'મુશ્કેલીઓ', 'નામ']]\n"
     ]
    }
   ],
   "source": [
    "# Step-4 Remove Single-letter words from the documents\n",
    "\n",
    "# ['તે', 'આ', 'તાજા'] ->  ['તાજા']\n",
    "\n",
    "\n",
    "slr_documents=[]\n",
    "for document in dgr_documents:\n",
    "    slr_documents.append(single_letter_word_removal(document))\n",
    "print('Total No. of tokens after single-letter words removal:',len(slr_documents))\n",
    "print(slr_documents[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for White Space Removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ગ્રેટર', 'નોઈડા', 'વેસ્ટની', 'મુશ્કેલીઓ', 'નામ', 'માંગણીઓને', 'નિર્માણકાર્ય', 'બંધ', 'ખેડૂત', 'સંગઠનોમાં', 'હરીફાઈ', 'શરૂ', 'રવિવારે', 'માયચા', 'ગામમાં', 'ખેડૂત', 'સંઘર્ષ', 'સમિતિએ', 'પંચાયત', 'બોલાવીને', 'ઓક્ટોબરથી', 'નિર્માણકાર્ય', 'બંધ', 'નિર્ણય', 'વળતર', 'આપવાના', 'ખેડૂતોની', 'પ્રાધિકરણ', 'એડીએમ', 'એલ', 'કાર્યાલય', 'કરોડ', 'રૂપિયા', 'ગામડાઓમાં', 'મજાક', 'પંચાયતમાં', 'ખેડૂતોએ', 'પ્રાધિકરણ', 'ખેડૂતોની', 'સમસ્યાઓ', 'ગંભીર', 'શાસન', 'સપ્ટેમ્બરની', 'વાતચીતમાં', 'ખેડૂતોને', 'ઓક્ટોબર', 'ગામડાઓમાં', 'વળતર', 'આપવાનું', 'આશ્વાસન', 'પંચાયતમાં', 'ખેડૂતોએ', 'નિર્ણય', 'ઓક્ટોબરે', 'ત્રણ', 'ચાર', 'ટીમ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટમાં', 'નિર્માણ', 'કાર્ય', 'બંધ', 'શેરડી', 'મગફળી', 'બટાકા', 'વગેરે', 'પાક', 'આફત', 'આવનારી', 'સફેદ', 'ઈયળ', 'સફેદ', 'લટ', 'સફેદ', 'ગિલારના', 'નામથી', 'ઓળખવામાં', 'અસાધ્ય', 'ભારતીય', 'કૃષિ', 'અનુસંધાન', 'પરિષદના', 'આઈસીએઆર', 'રાષ્ટ્રીય', 'કૃષિ', 'ઉપયોગી', 'કીટ', 'બ્યૂરોના', 'વૈજ્ઞાનિકોએ', 'પદ્ધતિ', 'વિકસાવી', 'રોગનો', 'ટકા', 'ઈલાજ', 'સંભવ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટની', 'મુશ્કેલીઓ', 'નામ']]\n"
     ]
    }
   ],
   "source": [
    "wsr_documents=[]\n",
    "wsr_documents = [[word.strip() for word in document] for document in slr_documents]\n",
    "print(wsr_documents[0:1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This cell of code is for writting back preprocessed file\n",
    "with open (r'C:\\Users\\Chauhan\\LemmaGuj\\tdil_combined\\preprocessed\\entertainment.txt',mode='w',errors = 'ignore', encoding = 'utf-8-sig') as fout:\n",
    "    for document in wsr_documents:\n",
    "        fout.write(\" \".join(document))\n",
    "        fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7786\n",
      "[['ગ્રેટર', 'નોઈડા', 'વેસ્ટની', 'મુશ્કેલીઓ', 'નામ', 'માંગણીઓને', 'નિર્માણકાર્ય', 'બંધ', 'ખેડૂત', 'સંગઠનોમાં', 'હરીફાઈ', 'શરૂ', 'રવિવારે', 'ગામમાં', 'ખેડૂત', 'સંઘર્ષ', 'સમિતિએ', 'પંચાયત', 'બોલાવીને', 'ઓક્ટોબરથી', 'નિર્માણકાર્ય', 'બંધ', 'નિર્ણય', 'વળતર', 'આપવાના', 'ખેડૂતોની', 'પ્રાધિકરણ', 'એલ', 'કાર્યાલય', 'કરોડ', 'રૂપિયા', 'ગામડાઓમાં', 'મજાક', 'પંચાયતમાં', 'ખેડૂતોએ', 'પ્રાધિકરણ', 'ખેડૂતોની', 'સમસ્યાઓ', 'ગંભીર', 'શાસન', 'સપ્ટેમ્બરની', 'વાતચીતમાં', 'ખેડૂતોને', 'ઓક્ટોબર', 'ગામડાઓમાં', 'વળતર', 'આપવાનું', 'આશ્વાસન', 'પંચાયતમાં', 'ખેડૂતોએ', 'નિર્ણય', 'ત્રણ', 'ચાર', 'ટીમ', 'ગ્રેટર', 'નોઈડા', 'નિર્માણ', 'કાર્ય', 'બંધ', 'શેરડી', 'મગફળી', 'બટાકા', 'વગેરે', 'પાક', 'આફત', 'આવનારી', 'સફેદ', 'ઈયળ', 'સફેદ', 'સફેદ', 'નામથી', 'ઓળખવામાં', 'અસાધ્ય', 'ભારતીય', 'કૃષિ', 'અનુસંધાન', 'પરિષદના', 'રાષ્ટ્રીય', 'કૃષિ', 'ઉપયોગી', 'કીટ', 'વૈજ્ઞાનિકોએ', 'પદ્ધતિ', 'વિકસાવી', 'રોગનો', 'ટકા', 'ઈલાજ', 'સંભવ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટની', 'મુશ્કેલીઓ', 'નામ']]\n"
     ]
    }
   ],
   "source": [
    "# Step-6 Remove less frequent (across the corpus) words from the document\n",
    "\n",
    "# frequency counts of each token in the corpus\n",
    "# printing them properly using pprint\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for document in slr_documents:\n",
    "    for token in document:\n",
    "        frequency[token] += 1\n",
    "\n",
    "        \n",
    "# Find all words which appears only 1 time in the corpus 'documents'\n",
    "preprocessed_documents = [[token for token in document if frequency[token] > 1] for document in wsr_documents]\n",
    "print(len(preprocessed_documents))\n",
    "#pprint(texts)\n",
    "print(preprocessed_documents[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('સમયે', 986)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(frequency)\n",
    "#print(dict(frequency))\n",
    "#print(list(frequency))\n",
    "list(frequency.items())[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization Component "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def lemmatize(unlemma_list):\n",
    "        '''\n",
    "        list[] -> list[]\n",
    "        (Read the list) -> (lemmatized list)\n",
    "\n",
    "        Read the word from list one at a time.\n",
    "        Remove the suffix from the word to convert that word to root word.\n",
    "        Check the lemmatized word in the Gujarati noun list. \n",
    "    \n",
    "        Precondition: List of words must be in the Gujarati language.\n",
    "        All words must be treated in UNICODE.\n",
    "        A list(Limited) of nouns of Gujarati language has been compiled.\n",
    "    \n",
    "        >>> lemmatize(['ગુજરાતની',' ગુજરાતી','ગુજરાતનું' ,'ગુજરાતનો','ગુજરાતિ','આપણું', 'આપણો', 'આપણી','આપણા','મારુ'])\n",
    "                >>>  ગુજરાતન  ગુજરાતી ગુજરાતન ગુજરાતનો ગુજરાત આપણ આપણો આપણ આપણ મારુ\n",
    "        \n",
    "        gujarati_noun_list = ['ગુજરાતની',' ગુજરાતી','ગુજરાતનું' ,'ગુજરાતિ','આપણું', 'આપણી','આપણા','મારુ']\n",
    "    \n",
    "        '''\n",
    "        lemma_list = []\n",
    "        #self.lemma1_list = []\n",
    "        regex_lemma = re.compile('(ની|ના|નો|માંથી|નું|નુ|નિ|નૂ|નૂં|ને|માં|મા|એ|ઓમાનું|ઓમાંનું|ઓમાંનુ|ઓમાનુ|માંથી|માથી|ઓને|માની|માંની|ઓ|થી)$',re.UNICODE)\n",
    "        #regex_lemma1 = re.compile('(ઇ)$',re.UNICODE)\n",
    "        for word in unlemma_list:\n",
    "            if (regex_lemma.sub(\"\",word)):\n",
    "                #if word in gujarati_noun_list:\n",
    "                lemma_list.append(regex_lemma.sub(\"\",word))\n",
    "            else:\n",
    "                lemma_list.append(word)\n",
    "                     \n",
    "        #for word in lemma_list:\n",
    "            #lemma1_list.append(regex_lemma1.sub('ઈ',word))\n",
    "            \n",
    "        return(lemma_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lemmaprocessed_documents=[]\n",
    "for document in preprocessed_documents:\n",
    "    lemmaprocessed_documents.append(lemmatize(document))\n",
    "print('Total No. of tokens after lemmatization:',len(lemmaprocessed_documents))\n",
    "print(lemmaprocessed_documents[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561648\n",
      "['નોઈડા', 'વેસ્ટની', 'મુશ્કેલીઓ', 'નામ', 'માંગણીઓને', 'નિર્માણકાર્ય', 'બંધ', 'ખેડૂત', 'સંગઠનોમાં']\n"
     ]
    }
   ],
   "source": [
    "# Merge all tokens of all documents in one list 'tokens[]'\n",
    "\n",
    "import itertools\n",
    "tokens = list(itertools.chain(*preprocessed_documents))\n",
    "print(len(tokens))\n",
    "print(tokens[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "dictionary.save('C:/Users/Chauhan/dictionaryTM/tdilTM.dict') # Store the dictionary for the future purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(42722 unique tokens: ['અનુસંધાન', 'અસાધ્ય', 'આપવાના', 'આપવાનું', 'આફત']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)\n",
    "#print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "import numpy\n",
    "from gensim.models import ldamodel\n",
    "%matplotlib inline\n",
    "\n",
    "dictionary = Dictionary(preprocessed_documents)\n",
    "corpus = [dictionary.doc2bow(document) for document in preprocessed_documents]\n",
    "#corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.54400825500488\n"
     ]
    }
   ],
   "source": [
    "# Topics are modeled for the document collection named 'corpus', vocabulary = 'dictionary', Topics to infer is 2\n",
    "numpy.random.seed(1) # Random seed set to 1, So that the randomization remains same for each experiment\n",
    "start = time.time()\n",
    "model = ldamodel.LdaModel(corpus,id2word=dictionary,num_topics=100,iterations=500)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-29.306563607314185\n"
     ]
    }
   ],
   "source": [
    "print(model.log_perplexity(corpus))\n",
    "#print(model.log_perplexity(lemmacorpus))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "#os.environ['MALLET_HOME'] = r'C:\\Users\\Chauhan\\mallet'\n",
    "os.environ.update({'MALLET_HOME':r'C:/Users/Chauhan/mallet/'}) \n",
    "\n",
    "mallet_path = 'C://Users//Chauhan//mallet//mallet//bin//mallet' # update this path\n",
    "#ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=dictionary)\n",
    "ldamallet = LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 0.08198782), (23, 0.7469607), (29, 0.015511116), (51, 0.089905925), (69, 0.038457014), (98, 0.013903983)]\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "get_document_topics = model.get_document_topics(corpus)\n",
    "print(get_document_topics[2])\n",
    "\n",
    "get_document_topics = [model.get_document_topics(item,minimum_probability=0.1, minimum_phi_value=None, per_word_topics=False) for item in corpus[0:1000]]\n",
    "for topic in get_document_topics:\n",
    "    topic\n",
    "print(len(get_document_topics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  4\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  4\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  4\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  3\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  4\n",
      "total dominating topics:  1\n",
      "total dominating topics:  4\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  4\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  4\n",
      "total dominating topics:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  4\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  4\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  4\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  2\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  4\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  3\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "total dominating topics:  2\n",
      "total dominating topics:  1\n",
      "average number of dominant topics 1.425\n"
     ]
    }
   ],
   "source": [
    "dominant_topics = []\n",
    "for i in range(len(get_document_topics)):\n",
    "    #print(i,get_document_topics[i])\n",
    "    dominant_topics.append(len(get_document_topics[i]))\n",
    "    print(\"total dominating topics: \", len(get_document_topics[i]))\n",
    "print('average number of dominant topics',sum(dominant_topics)/len(dominant_topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topic proportion in the first N (e.g. N=20) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Retriving the topic proportion in the first N (e.g. N=20) document of the corpus\n",
    "# To assess the proportion of topics spread over document\n",
    "\n",
    "topic_proportion = numpy.zeros((10,100))\n",
    "print(topic_proportion)\n",
    "for i,document in zip(range(0,10),get_document_topics):\n",
    "#for document in get_document_topics:\n",
    "    for item in document:\n",
    "        index,prop = item\n",
    "        #print(index,prop*100)\n",
    "        topic_proportion[i][index] = round(prop*100,3)\n",
    "print('new topic proportion\\n',topic_proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the topic Proportion ( Stacked bar chart)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "N, K = topic_proportion.shape\n",
    "docnames = ['Doc #{}'.format(n) for n in range(N)]\n",
    "ind = np.arange(N) # the x-axis locations for the novels\n",
    "width = 0.5 # the width of the bars\n",
    "plots = []\n",
    "height_cumulative = np.zeros(N)\n",
    "\n",
    "for k in range(K):\n",
    "    color = plt.cm.coolwarm(k/K, 1)\n",
    "    if k == 0:\n",
    "        p = plt.bar(ind, topic_proportion[:, k], width, color=color)\n",
    "    else:\n",
    "        p = plt.bar(ind, topic_proportion[:, k], width, bottom=height_cumulative, color=color)\n",
    "    height_cumulative += topic_proportion[:, k]\n",
    "    plots.append(p)\n",
    "\n",
    "#plt.ylim((0, 1)) # proportions sum to 1, so the height of the stacked bars is 1\n",
    "plt.ylabel('Topic Proportion',fontsize=14)\n",
    "plt.xlabel('Documents',fontsize=14)\n",
    "plt.title('Topic in the Corpus',fontsize=18)\n",
    "plt.xticks(ind+width/26, docnames,rotation=90)\n",
    "#plt.yticks(np.arange(0, 1, 10))\n",
    "topic_labels = ['Topic #{}'.format(k) for k in range(K)]\n",
    "plt.legend([p[0] for p in plots], topic_labels,loc='upper left', bbox_to_anchor=(1, 1),fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the share of topic 0 for first 10 document in the corpus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.ylabel('Topic Proportion',fontsize=14)\n",
    "plt.xlabel('Documents',fontsize=14)\n",
    "plt.bar(ind,topic_proportion[:,1], width=width)\n",
    "plt.xticks(ind+width/4, docnames,rotation=90)\n",
    "plt.title('Share of Topic #0',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the share of a few topics for first 10 document in the corpus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3,figsize=(9, 9),sharex=True, sharey=True,\n",
    "                        subplot_kw={'xticks': [x for x in range(0,10,1)],'yticks': [y for y in range(10,110,20)]})\n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "fig.suptitle('Topic share for the set of documents',fontsize=18)\n",
    "fig.text(0.5,-0.02, \"Set of 10 documents\", ha=\"center\", va=\"center\",fontsize=16)\n",
    "fig.text(-0.02,0.5, \"Topic proportion in documents\", ha=\"center\", va=\"center\", rotation=90,fontsize=16)\n",
    "#ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tick_params(labelsize=14)\n",
    "\n",
    "topics = []\n",
    "\n",
    "for i in range(1,10):\n",
    "    topics.append('Topic #'+str(i))\n",
    "print(topics)\n",
    "\n",
    "\n",
    "for ax,i,topic in zip(axs.flat,range(1,10),topics):\n",
    "    ax.bar(ind,topic_proportion[:,i], width=width)\n",
    "    ax.set_title(topic)\n",
    "    ax.tick_params(labelsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat Map for topic and document"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# To plot heat map\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.yticks(np.arange(topic_proportion.shape[0])+0.5, docnames)\n",
    "plt.xticks(np.arange(topic_proportion.shape[1])+0.5, topic_labels)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xticks(rotation=90)\n",
    "plt.pcolor(topic_proportion, norm=None, cmap='Blues')\n",
    "plt.colorbar(cmap='Blues')\n",
    "plt.ylabel('Documents',fontsize=14)\n",
    "plt.xlabel('Topics',fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.show_topics(num_topics=100,num_words=100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(font_path='lohit_gu.ttf',\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=30,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = model.show_topics(num_words=30,formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=600)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=30))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatized Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No. of tokens after tokenization: 7798\n",
      "[['ગ્રેટર', 'નોઈડા', 'વેસ્ટ', 'મુશ્કેલી', 'નામ', 'માંગણી', 'નિર્માણકાર્ય', 'બંધ', 'ખેડૂત', 'સંગઠન', 'હરીફાઈ', 'શરૂ', 'રવિવાર', 'માયચા', 'ગામ', 'ખેડૂત', 'સંઘર્ષ', 'સમિતિ', 'પંચાયત', 'બોલાવી', 'ઓક્ટોબર', 'નિર્માણકાર્ય', 'બંધ', 'નિર્ણય', 'વળતર', 'આપ', 'ખેડૂત', 'પ્રાધિકરણ', 'એડીએમ', 'એલ', 'કાર્યાલય', 'કરોડ', 'રૂપિયો', 'ગામડા', 'મજાક', 'પંચાયત', 'ખેડૂત', 'પ્રાધિકરણ', 'ખેડૂત', 'સમસ્યા', 'ગંભીર', 'શાસન', 'સપ્ટેમ્બર', 'વાતચીત', 'ખેડૂત', 'ઓક્ટોબર', 'ગામડા', 'વળતર', 'આપવા', 'આશ્વાસન', 'પંચાયત', 'ખેડૂત', 'નિર્ણય', 'ઓક્ટોબર', 'ત્રણ', 'ચાર', 'ટીમ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટ', 'નિર્માણ', 'કાર્ય', 'બંધ', 'શેરડી', 'મગફળી', 'બટાકા', 'વગેર', 'પાક', 'આફત', 'આવનારી', 'સફેદ', 'ઈયળ', 'સફેદ', 'લટ', 'સફેદ', 'ગિલાર', 'નામ', 'ઓળખ', 'અસાધ્ય', 'ભારતીય', 'કૃષિ', 'અનુસંધાન', 'પરિષદ', 'આઈસીએઆર', 'રાષ્ટ્રીય', 'કૃષિ', 'ઉપયોગી', 'કીટ', 'બ્યૂર', 'વૈજ્ઞાનિક', 'પદ્ધતિ', 'વિકસાવી', 'રોગ', 'ટકા', 'ઈલાજ', 'સંભવ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટ', 'મુશ્કેલી', 'નામ']]\n"
     ]
    }
   ],
   "source": [
    "inter_lemmaprocessed_documents = []\n",
    "content = open(r'C:/Users/Chauhan/LemmaGuj/tdil_combined/lemmatized11july/all/all1.txt',encoding='utf-8-sig').readlines()\n",
    "for line in content:\n",
    "    inter_lemmaprocessed_documents.append([word for word in nltk.word_tokenize(line)])\n",
    "print('Total No. of tokens after tokenization:',len(inter_lemmaprocessed_documents))\n",
    "print(inter_lemmaprocessed_documents[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7798\n",
      "[['ગ્રેટર', 'નોઈડા', 'વેસ્ટ', 'મુશ્કેલી', 'નામ', 'માંગણી', 'નિર્માણકાર્ય', 'બંધ', 'ખેડૂત', 'સંગઠન', 'હરીફાઈ', 'શરૂ', 'રવિવાર', 'ગામ', 'ખેડૂત', 'સંઘર્ષ', 'સમિતિ', 'પંચાયત', 'બોલાવી', 'ઓક્ટોબર', 'નિર્માણકાર્ય', 'બંધ', 'નિર્ણય', 'વળતર', 'આપ', 'ખેડૂત', 'પ્રાધિકરણ', 'એલ', 'કાર્યાલય', 'કરોડ', 'રૂપિયો', 'ગામડા', 'મજાક', 'પંચાયત', 'ખેડૂત', 'પ્રાધિકરણ', 'ખેડૂત', 'સમસ્યા', 'ગંભીર', 'શાસન', 'સપ્ટેમ્બર', 'વાતચીત', 'ખેડૂત', 'ઓક્ટોબર', 'ગામડા', 'વળતર', 'આપવા', 'આશ્વાસન', 'પંચાયત', 'ખેડૂત', 'નિર્ણય', 'ઓક્ટોબર', 'ત્રણ', 'ચાર', 'ટીમ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટ', 'નિર્માણ', 'કાર્ય', 'બંધ', 'શેરડી', 'મગફળી', 'બટાકા', 'વગેર', 'પાક', 'આફત', 'આવનારી', 'સફેદ', 'ઈયળ', 'સફેદ', 'સફેદ', 'નામ', 'ઓળખ', 'અસાધ્ય', 'ભારતીય', 'કૃષિ', 'અનુસંધાન', 'પરિષદ', 'આઈસીએઆર', 'રાષ્ટ્રીય', 'કૃષિ', 'ઉપયોગી', 'કીટ', 'બ્યૂર', 'વૈજ્ઞાનિક', 'પદ્ધતિ', 'વિકસાવી', 'રોગ', 'ટકા', 'ઈલાજ', 'સંભવ', 'ગ્રેટર', 'નોઈડા', 'વેસ્ટ', 'મુશ્કેલી', 'નામ']]\n"
     ]
    }
   ],
   "source": [
    "# Step-6 Remove less frequent (across the corpus) words from the document\n",
    "\n",
    "# frequency counts of each token in the corpus\n",
    "# printing them properly using pprint\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for document in inter_lemmaprocessed_documents:\n",
    "    for token in document:\n",
    "        frequency[token] += 1\n",
    "\n",
    "        \n",
    "# Find all words which appears only 1 time in the corpus 'documents'\n",
    "lemmaprocessed_documents = [[token for token in document if frequency[token] > 1] for document in inter_lemmaprocessed_documents]\n",
    "print(len(lemmaprocessed_documents))\n",
    "#pprint(texts)\n",
    "print(lemmaprocessed_documents[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580628\n",
      "['નોઈડા', 'વેસ્ટ', 'મુશ્કેલી', 'નામ', 'માંગણી', 'નિર્માણકાર્ય', 'બંધ', 'ખેડૂત', 'સંગઠન']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "tokens = list(itertools.chain(*lemmaprocessed_documents))\n",
    "print(len(tokens))\n",
    "print(tokens[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "lemmadictionary = corpora.Dictionary(lemmaprocessed_documents)\n",
    "dictionary.save('C:/Users/Chauhan/dictionaryTM/tdillemmaTM.dict.dict') # Store the dictionary for the future purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(26882 unique tokens: ['અનુસંધાન', 'અસાધ્ય', 'આઈસીએઆર', 'આપ', 'આપવા']...)\n"
     ]
    }
   ],
   "source": [
    "print(lemmadictionary)\n",
    "#print(lemmadictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "import numpy\n",
    "from gensim.models import ldamodel\n",
    "%matplotlib inline\n",
    "\n",
    "lemmadictionary = Dictionary(lemmaprocessed_documents)\n",
    "lemmacorpus = [lemmadictionary.doc2bow(document) for document in lemmaprocessed_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lemmadictionary)\n",
    "#print(lemmadictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.520277976989746\n"
     ]
    }
   ],
   "source": [
    "# Topics are modeled for the document collection named 'corpus', vocabulary = 'dictionary', Topics to infer is 2\n",
    "numpy.random.seed(1) # Random seed set to 1, So that the randomization remains same for each experiment\n",
    "start = time.time()\n",
    "lemmamodel = ldamodel.LdaModel(lemmacorpus,id2word=lemmadictionary,num_topics=100,iterations=500)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-22.77545657642724\n"
     ]
    }
   ],
   "source": [
    "print(lemmamodel.log_perplexity(lemmacorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0.038308416), (9, 0.098571755), (12, 0.069325216), (15, 0.5894349), (29, 0.016779803), (61, 0.13847804), (85, 0.0359624)]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "lemma_get_document_topics = lemmamodel.get_document_topics(corpus)\n",
    "print(lemma_get_document_topics[2])\n",
    "\n",
    "lemma_get_document_topics = [lemmamodel.get_document_topics(item,minimum_probability=0.1, minimum_phi_value=None, per_word_topics=False) for item in corpus[0:1000]]\n",
    "for topic in lemma_get_document_topics:\n",
    "    topic\n",
    "print(len(lemma_get_document_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of dominant topics 2.42\n"
     ]
    }
   ],
   "source": [
    "lemma_dominant_topics = []\n",
    "for i in range(len(lemma_get_document_topics)):\n",
    "    #print(i,lemma_get_document_topics[i])\n",
    "    lemma_dominant_topics.append(len(lemma_get_document_topics[i]))\n",
    "    #print(\"total dominating topics: \", len(lemma_get_document_topics[i]))\n",
    "print('average number of dominant topics',sum(lemma_dominant_topics)/len(lemma_dominant_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lemmamodel.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Defining Global Topic ( Overly Topic)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Step 1. Preparing frequency distribution of words in the corpus.\n",
    "#Step 2. Replace word by tokenid using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 26882 samples and 580628 outcomes>\n",
      "26882\n"
     ]
    }
   ],
   "source": [
    "# The frequency of each token in the corpus\n",
    "tokenfreq = nltk.FreqDist(tokens)\n",
    "print(tokenfreq)\n",
    "print(len(tokenfreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('રોગ', 3365),\n",
       " ('સમય', 2786),\n",
       " ('પાણી', 2583),\n",
       " ('ઉપયોગ', 2472),\n",
       " ('વર્ષ', 2331),\n",
       " ('મંદિર', 2283),\n",
       " ('દૂર', 2227),\n",
       " ('લોક', 2135),\n",
       " ('ખૂબ', 1938),\n",
       " ('બાળક', 1929),\n",
       " ('શરીર', 1890),\n",
       " ('સ્થળ', 1879),\n",
       " ('દિવસ', 1839),\n",
       " ('ભાગ', 1749),\n",
       " ('મુખ્ય', 1631),\n",
       " ('દેશ', 1615),\n",
       " ('લગભગ', 1591),\n",
       " ('ભારત', 1553),\n",
       " ('વગેર', 1551),\n",
       " ('લાગ', 1484),\n",
       " ('વિસ્તાર', 1382),\n",
       " ('રૂપ', 1340),\n",
       " ('સ્ત્રી', 1335),\n",
       " ('ઉત્પાદન', 1284),\n",
       " ('જોવા', 1281),\n",
       " ('દવા', 1253),\n",
       " ('દરદી', 1231),\n",
       " ('સામાન્ય', 1200),\n",
       " ('કિલોમીટર', 1185),\n",
       " ('ખેતી', 1177),\n",
       " ('શહેર', 1162),\n",
       " ('નામ', 1156),\n",
       " ('ફિલ્મ', 1153),\n",
       " ('લોહી', 1144),\n",
       " ('પાક', 1132),\n",
       " ('સુંદર', 1098),\n",
       " ('કામ', 1079),\n",
       " ('ત્રણ', 1046),\n",
       " ('છોડ', 1020),\n",
       " ('તેલ', 1004),\n",
       " ('રાષ્ટ્રીય', 1003),\n",
       " ('સિવાય', 983),\n",
       " ('વાર', 983),\n",
       " ('સ્થિતિ', 961),\n",
       " ('પર્યટક', 950),\n",
       " ('પેટ', 933),\n",
       " ('ફળ', 921),\n",
       " ('માત્ર', 917),\n",
       " ('વ્યક્તિ', 912),\n",
       " ('સારવાર', 896),\n",
       " ('અહીં', 880),\n",
       " ('ઘણી', 872),\n",
       " ('નજીક', 860),\n",
       " ('હાથ', 858),\n",
       " ('ચામડી', 855),\n",
       " ('વચ્ચ', 854),\n",
       " ('જમીન', 847),\n",
       " ('આંખ', 841),\n",
       " ('ઉદ્યાન', 839),\n",
       " ('નદી', 833),\n",
       " ('સરોવર', 821),\n",
       " ('અંદર', 813),\n",
       " ('પગ', 807),\n",
       " ('દરેક', 805),\n",
       " ('વાત', 803),\n",
       " ('રંગ', 800),\n",
       " ('ખાસ', 796),\n",
       " ('ધ્યાન', 780),\n",
       " ('શરૂ', 778),\n",
       " ('પ્રમાણ', 761),\n",
       " ('પ્રાચીન', 761),\n",
       " ('સમસ્યા', 759),\n",
       " ('ભોજન', 756),\n",
       " ('વિકાસ', 754),\n",
       " ('જરૂર', 746),\n",
       " ('વિશેષ', 744),\n",
       " ('રાજ્ય', 741),\n",
       " ('સારી', 741),\n",
       " ('દુખાવ', 740),\n",
       " ('ઓછું', 734),\n",
       " ('અસર', 731),\n",
       " ('સ્થાન', 718),\n",
       " ('કિ.મ', 708),\n",
       " ('ભારતીય', 707),\n",
       " ('ખા', 702),\n",
       " ('પહેલાં', 699),\n",
       " ('ખેડૂત', 690),\n",
       " ('ગરમ', 682),\n",
       " ('ચાર', 680),\n",
       " ('કારણક', 680),\n",
       " ('કૃષિ', 670),\n",
       " ('રોગી', 665),\n",
       " ('ઓછા', 664),\n",
       " ('ગામ', 662),\n",
       " ('લક્ષણ', 659),\n",
       " ('તૈયાર', 657),\n",
       " ('કલાક', 655),\n",
       " ('ઉત્પન્ન', 654),\n",
       " ('અંતર', 653),\n",
       " ('ક્ષેત્ર', 652)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the top 1o words with their corresponding frequency\n",
    "tokenfreq.most_common(100)\n",
    "#type(tokenfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(863, 0.005795449065494602), (167, 0.004798252926142039), (886, 0.004448631481774906), (474, 0.004257459164904207), (206, 0.004014618654284671), (2412, 0.003931949544286531), (286, 0.0038355022492887014), (9415, 0.0036770531217922664), (137, 0.0033377653161749004), (12627, 0.003322264858050249)]\n"
     ]
    }
   ],
   "source": [
    "# Preparing Global Topic (The Zipf's distribution)\n",
    "# Replace word by the tokenid (Mapping from dictionary using doc2bow)\n",
    "# Replacing count by corresponding probability values (Dividing the frequncy by total number of tokens)\n",
    "\n",
    "global_topic = []\n",
    "freqlen = len(tokenfreq)\n",
    "for item in tokenfreq.most_common(10):\n",
    "    #print(item)\n",
    "    word,freq = item\n",
    "    # get rid of spaces and quote marks\n",
    "    word = word.replace(\" \",\"\").replace('\"', '')\n",
    "    #print(word,freq)\n",
    "    word = model.id2word.doc2bow([word])[0][0]\n",
    "    global_topic.append((word,float(freq/len(tokens))))\n",
    "#print('ok')\n",
    "print(global_topic)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load the first two topics from the model to the variable topic_1 and topic_2\n",
    "# for Large number of topic, They must be loaded iteratively\n",
    "\n",
    "topic_1, topic_2 = model.show_topics(num_topics=2,num_words=10)\n",
    "print(topic_1,topic_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming topics to a format, such that the distance can be computed\n",
    "# Input format - (0, '0.123*\"star\" + 0.106*\"spend\" + 0.104*\"wars\" + 0.077*\"ap\" + 0.073*\"retailers\"') \n",
    "# Output format - [(14, 0.123), (17, 0.106), (15, 0.104), (2, 0.077), (13, 0.073)]\n",
    "# We have alrady transformed corpus wise frequency distribution of words to the compatible format\n",
    "\n",
    "\n",
    "def parse_topic_string(topic):\n",
    "    # takes the string returned by model.show_topics()\n",
    "    # split on strings to get topics and the probabilities\n",
    "    topic = topic.split('+')\n",
    "    # list to store topic bows\n",
    "    topic_bow = []\n",
    "    for word in topic:\n",
    "        # split probability and word\n",
    "        prob, word = word.split('*')\n",
    "        # get rid of spaces and quote marks\n",
    "        word = word.replace(\" \",\"\").replace('\"', '')\n",
    "        # convert to word_type\n",
    "        word = model.id2word.doc2bow([word])[0][0]\n",
    "        topic_bow.append((word, float(prob)))\n",
    "    return topic_bow\n",
    "\n",
    "#topic_1_distribution = parse_topic_string(topic_1[1])\n",
    "#topic_2_distribution = parse_topic_string(topic_2[1])\n",
    "\n",
    "# the finance topic in bag of words format looks like this:\n",
    "#print(topic_1_distribution)\n",
    "#print(topic_2_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import hellinger\n",
    "from gensim.matutils import kullback_leibler"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Distance between two topics :' , hellinger(topic_1_distribution,topic_2_distribution))\n",
    "print('Distance of topc_1 from Global topic :', hellinger(topic_1_distribution,global_topic))\n",
    "print('Distance of topc_2 from Global topic :', hellinger(topic_2_distribution,global_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "topics = model.show_topics(num_topics=100,num_words=50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distribution=[]\n",
    "for topic in topics:\n",
    "    topic_distribution.append(parse_topic_string(topic[1]))\n",
    "#print(topic_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model average distance 0.46503096615443923\n",
      "[0.46065973287960466, 0.497126344507516, 0.48636227251169944, 0.4664285184074615, 0.44683986310317847, 0.4818050074840041, 0.5063574760586753, 0.47086035423583567, 0.49854949788333003, 0.48455851035074504, 0.4541029323744746, 0.40985995358295474, 0.5241273444413072, 0.47350762738428687, 0.5061170179744505, 0.4854560867329246, 0.4644222405296629, 0.4224734169375327, 0.48376432056400687, 0.48586315717677053, 0.45536265944542487, 0.5514028418091511, 0.45275528876989163, 0.5007286416941331, 0.4839519327279277, 0.44217141032518437, 0.39760587000515096, 0.48782165413250267, 0.35812367483975927, 0.412708723229952, 0.43765518794751274, 0.47143415180573484, 0.47766343683300033, 0.45839412722046524, 0.5016006414347373, 0.47862300155248677, 0.5250239501887262, 0.4129361624064132, 0.5315833384071386, 0.4655206474380021, 0.4543423965350699, 0.3910330032734523, 0.46199458228803625, 0.4533674707534416, 0.4917362820829556, 0.38337653294332313, 0.42421564846030874, 0.39683838511381336, 0.4373700784506595, 0.49901136498387927, 0.41137582697891506, 0.4590662844262914, 0.5056349790477631, 0.5199292466450245, 0.4724914073221863, 0.3912744017128189, 0.48159514436373557, 0.47350762738428687, 0.40880836239474017, 0.5494319175570503, 0.49067866069272426, 0.5418076806245733, 0.42011120821310133, 0.41998094318664714, 0.42796579005196966, 0.521576517432843, 0.5307909638322673, 0.5063716867173013, 0.446245837007441, 0.5332085758280385, 0.46368715562760343, 0.4792801614829229, 0.4279164126738901, 0.3833326463816751, 0.5123767178546713, 0.4558347100242598, 0.552416132648734, 0.5217676988441005, 0.46474013205698944, 0.472765621466496, 0.38001473395986307, 0.45662813978948535, 0.5017359283430275, 0.49085916699551185, 0.43527213027739936, 0.41514327193957135, 0.4183815581335164, 0.38166425758600736, 0.4948745111072337, 0.4561043641516536, 0.47168532918841954, 0.4480631896394761, 0.45508510231272437, 0.49454150389588636, 0.42041685595184664, 0.45629976242717535, 0.4534358391537624, 0.5349282227822781, 0.4585508221164091, 0.42784671299294713]\n"
     ]
    }
   ],
   "source": [
    "hellinger_distance = []\n",
    "kullback_distance = []\n",
    "avg = 0\n",
    "\n",
    "for topic in topic_distribution:\n",
    "    hellinger_distance.append(hellinger(topic,global_topic))\n",
    "    #kullback_distance.append(kullback_leibler(topic,global_topic))\n",
    "    #for (distance,i) in zip(hellinger_distance,range(len(hellinger_distance))):\n",
    "    #print(\"distance between topic\", i , \"and global topic is\", distance)\n",
    "    \n",
    "print('model average distance', sum(hellinger_distance)/len(hellinger_distance))\n",
    "#print('model average distance', sum(kullback_distance)/len(kullback_distance))\n",
    "print(hellinger_distance[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model average jaccard distance 0.9793022846716477\n",
      "[0.9837126272055547, 0.9924855801759037, 0.9849764932942773, 0.9924556456194543, 0.9811383127381968, 0.9652539019955276, 0.9789936910587311, 1.0, 0.968860290462828, 0.985355686132768]\n"
     ]
    }
   ],
   "source": [
    "# from gensim.matutils import kullback_leibler\n",
    "# kullback_distance = []\n",
    "# avg = 0\n",
    "\n",
    "\n",
    "# for topic in topic_distribution:\n",
    "#     kullback_distance.append(kullback_leibler(topic,global_topic))\n",
    "    \n",
    "\n",
    "from gensim.matutils import jaccard\n",
    "jaccard_distance = []\n",
    "avg = 0\n",
    "\n",
    "\n",
    "for topic in topic_distribution:\n",
    "    jaccard_distance.append(jaccard(topic,global_topic))\n",
    "print('model average jaccard distance', sum(jaccard_distance)/len(jaccard_distance))\n",
    "print(jaccard_distance[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sematic coherence for method u_mass is -9.921127019443418\n",
      "Sematic coherence for method c_v is 0.4078451392816971\n",
      "Sematic coherence for method c_uci is -7.264971483225013\n",
      "Sematic coherence for method c_npmi is -0.24390785724887132\n"
     ]
    }
   ],
   "source": [
    "# Computing semantic coherence of the model\n",
    "\n",
    "coherence_methods = ['u_mass', 'c_v', 'c_uci', 'c_npmi']\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "for coherence_method in coherence_methods:\n",
    "    cm = CoherenceModel(model=model, texts=preprocessed_documents, coherence=coherence_method) #  reference corpus is texts\n",
    "    coherence = cm.get_coherence()\n",
    "    print(\"Sematic coherence for method {} is {}\".format(coherence_method,coherence))\n",
    "\n",
    "#cm = CoherenceModel(model=model, corpus=lemmacorpus, coherence='u_mass') # reference corpus is corpus \n",
    "#coherence = cm.get_coherence()\n",
    "#coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_umass = CoherenceModel(model=model, texts=preprocessed_documents, coherence='u_mass')\n",
    "cm_cuci = CoherenceModel(model=model, texts=preprocessed_documents, coherence='c_uci')\n",
    "cm_cnpmi = CoherenceModel(model=model, texts=preprocessed_documents, coherence='c_npmi')\n",
    "coherence_umass = cm_umass.get_coherence()\n",
    "coherence_cuci = cm_cuci.get_coherence()\n",
    "coherence_cm_cnpmi = cm_cnpmi.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_umass_list,coherence_umass_list,coherence_cnpmi_list = [],[],[]\n",
    "coherence_umass_list = cm_umass.get_coherence_per_topic()\n",
    "coherence_cuci_list = cm_cuci.get_coherence_per_topic()\n",
    "coherence_cnpmi_list = cm_cnpmi.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence_umass_list:  [-14.50732557198049, -7.677690225672174, -4.950073603525697, -13.689022156460405, -10.2370267112648, -5.765697987463665, -10.578221867991532, -8.538247106811308, -6.157964054488857, -5.034660281859198, -16.51673094353111, -9.04866949120235, -9.501784360766115, -9.174622971242488, -7.750726867214081, -11.898990307773367, -9.552195766674897, -9.08179212964334, -9.478940428778937, -11.414970582606552]\n",
      "coherence_cuci_list:  [-10.039600215038185, -6.844224742037902, -3.280326200942328, -10.104592035584277, -8.408780649555426, -3.8497815024915996, -7.591710029897776, -7.202537753927328, -3.802017260704364, -2.8577847068832942, -10.853296455060258, -7.59913736974818, -7.569460064427403, -7.917652241350981, -5.921903731783221, -8.678542772964043, -7.3092131343127225, -6.951235230253199, -7.564727630339626, -7.4959407605391215]\n",
      "coherence_cnpmi_list:  [-0.3485967053616946, -0.2280079685363033, -0.08186248802962151, -0.34727354476182454, -0.2775722227050592, -0.12182595489764632, -0.2547330928766012, -0.23262417617370276, -0.09397072034136024, -0.07368853864406884, -0.3775379613278873, -0.26925304395096455, -0.26534095692698006, -0.2742716321909808, -0.1967240222230136, -0.29852180475516693, -0.250230345589274, -0.24704431201982244, -0.2596057855876578, -0.24526280497413114]\n"
     ]
    }
   ],
   "source": [
    "print('coherence_umass_list: ', coherence_umass_list[0:20])\n",
    "print('coherence_cuci_list: ', coherence_cuci_list[0:20])\n",
    "print('coherence_cnpmi_list: ', coherence_cnpmi_list[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatized Topic Distribution distance computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26882 580628\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "lemmatokens = list(itertools.chain(*lemmaprocessed_documents))\n",
    "print(len(set(lemmatokens)),len(lemmatokens))\n",
    "#print(tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 26882 samples and 580628 outcomes>\n",
      "26882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('રોગ', 3365),\n",
       " ('સમય', 2786),\n",
       " ('પાણી', 2583),\n",
       " ('ઉપયોગ', 2472),\n",
       " ('વર્ષ', 2331),\n",
       " ('મંદિર', 2283),\n",
       " ('દૂર', 2227),\n",
       " ('લોક', 2135),\n",
       " ('ખૂબ', 1938),\n",
       " ('બાળક', 1929),\n",
       " ('શરીર', 1890),\n",
       " ('સ્થળ', 1879),\n",
       " ('દિવસ', 1839),\n",
       " ('ભાગ', 1749),\n",
       " ('મુખ્ય', 1631),\n",
       " ('દેશ', 1615),\n",
       " ('લગભગ', 1591),\n",
       " ('ભારત', 1553),\n",
       " ('વગેર', 1551),\n",
       " ('લાગ', 1484),\n",
       " ('વિસ્તાર', 1382),\n",
       " ('રૂપ', 1340),\n",
       " ('સ્ત્રી', 1335),\n",
       " ('ઉત્પાદન', 1284),\n",
       " ('જોવા', 1281),\n",
       " ('દવા', 1253),\n",
       " ('દરદી', 1231),\n",
       " ('સામાન્ય', 1200),\n",
       " ('કિલોમીટર', 1185),\n",
       " ('ખેતી', 1177),\n",
       " ('શહેર', 1162),\n",
       " ('નામ', 1156),\n",
       " ('ફિલ્મ', 1153),\n",
       " ('લોહી', 1144),\n",
       " ('પાક', 1132),\n",
       " ('સુંદર', 1098),\n",
       " ('કામ', 1079),\n",
       " ('ત્રણ', 1046),\n",
       " ('છોડ', 1020),\n",
       " ('તેલ', 1004),\n",
       " ('રાષ્ટ્રીય', 1003),\n",
       " ('સિવાય', 983),\n",
       " ('વાર', 983),\n",
       " ('સ્થિતિ', 961),\n",
       " ('પર્યટક', 950),\n",
       " ('પેટ', 933),\n",
       " ('ફળ', 921),\n",
       " ('માત્ર', 917),\n",
       " ('વ્યક્તિ', 912),\n",
       " ('સારવાર', 896)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatokenfreq = nltk.FreqDist(lemmatokens)\n",
    "print(lemmatokenfreq)\n",
    "print(len(lemmatokenfreq))\n",
    "lemmatokenfreq.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatopics = []\n",
    "lemmatopics = lemmamodel.show_topics(num_topics=100,num_words=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(50, 0.005795449065494602), (110, 0.004798252926142039), (690, 0.004448631481774906), (74, 0.004257459164904207), (145, 0.004014618654284671), (1761, 0.003931949544286531), (248, 0.0038355022492887014), (430, 0.0036770531217922664), (121, 0.0033377653161749004), (778, 0.003322264858050249)]\n"
     ]
    }
   ],
   "source": [
    "lemmaglobal_topic = []\n",
    "lemmafreqlen = len(lemmatokenfreq)\n",
    "for item in lemmatokenfreq.most_common(10):\n",
    "    #print(item)\n",
    "    word,freq = item\n",
    "    # get rid of spaces and quote marks\n",
    "    word = word.replace(\" \",\"\").replace('\"', '')\n",
    "    #print(word,freq)\n",
    "    word = lemmamodel.id2word.doc2bow([word])[0][0]\n",
    "    lemmaglobal_topic.append((word,float(freq/len(lemmatokens))))\n",
    "#print('ok')\n",
    "print(lemmaglobal_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_topic_string(topic):\n",
    "    # takes the string returned by model.show_topics()\n",
    "    # split on strings to get topics and the probabilities\n",
    "    topic = topic.split('+')\n",
    "    # list to store topic bows\n",
    "    topic_bow = []\n",
    "    for word in topic:\n",
    "        # split probability and word\n",
    "        prob, word = word.split('*')\n",
    "        # get rid of spaces and quote marks\n",
    "        word = word.replace(\" \",\"\").replace('\"', '')\n",
    "        # convert to word_type\n",
    "        word = lemmamodel.id2word.doc2bow([word])[0][0]\n",
    "        topic_bow.append((word, float(prob)))\n",
    "    return topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatopic_distribution=[]\n",
    "for topic in lemmatopics:\n",
    "    lemmatopic_distribution.append(parse_topic_string(topic[1]))\n",
    "#print(lemmatopic_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma model average distance 0.4866254077132545\n"
     ]
    }
   ],
   "source": [
    "# Compute the distance between lemma topics and lemma global topic\n",
    "\n",
    "lemmahellinger_distance = []\n",
    "\n",
    "for topic in lemmatopic_distribution:\n",
    "    lemmahellinger_distance.append(hellinger(topic,lemmaglobal_topic))\n",
    "    \n",
    "print('lemma model average distance', sum(lemmahellinger_distance)/len(lemmahellinger_distance))\n",
    "\n",
    "#for (distance,i) in zip(lemmahellinger_distance,range(len(lemmahellinger_distance))):\n",
    "    #print(\"distance between topic\", i , \"and lemmaglobal topic is\", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma model Jaccard average distance 0.9995072125920921\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9886838411267271, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.993382950144688, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9901662374721519, 1.0, 1.0, 1.0, 1.0, 0.993008728829327, 1.0, 0.990724105580213, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9947553960560903, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import jaccard\n",
    "lemmajaccard_distance = []\n",
    "avg = 0\n",
    "\n",
    "\n",
    "for topic in lemmatopic_distribution:\n",
    "    lemmajaccard_distance.append(jaccard(topic,global_topic))\n",
    "print('lemma model Jaccard average distance', sum(lemmajaccard_distance)/len(lemmajaccard_distance))\n",
    "print(lemmajaccard_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized - Sematic coherence for method u_mass is -8.249185006208906\n",
      "Lemmatized - Sematic coherence for method c_v is 0.4026439475443246\n",
      "Lemmatized - Sematic coherence for method c_uci is -5.928855515432373\n",
      "Lemmatized - Sematic coherence for method c_npmi is -0.1956450471271498\n"
     ]
    }
   ],
   "source": [
    "# Computing semantic coherence of the lemmamodel\n",
    "\n",
    "coherence_methods = ['u_mass', 'c_v', 'c_uci', 'c_npmi']\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "for coherence_method in coherence_methods:\n",
    "    cm = CoherenceModel(model=lemmamodel, texts=lemmaprocessed_documents, coherence=coherence_method) #  reference corpus is texts\n",
    "    coherence = cm.get_coherence()\n",
    "    print(\"Lemmatized - Sematic coherence for method {} is {}\".format(coherence_method,coherence))\n",
    "\n",
    "# from gensim.models.coherencemodel import CoherenceModel\n",
    "# lemmacm = CoherenceModel(model=lemmamodel, corpus=lemmacorpus, coherence='u_mass')\n",
    "# coherence = lemmacm.get_coherence()\n",
    "# coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmacm_umass = CoherenceModel(model=lemmamodel, texts=lemmaprocessed_documents, coherence='u_mass')\n",
    "lemmacm_cuci = CoherenceModel(model=lemmamodel, texts=lemmaprocessed_documents, coherence='c_uci')\n",
    "lemmacm_cnpmi = CoherenceModel(model=lemmamodel, texts=lemmaprocessed_documents, coherence='c_npmi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmacoherence_umass_list,lemmacoherence_umass_list,lemmacoherence_cnpmi_list = [],[],[]\n",
    "lemmacoherence_umass_list = lemmacm_umass.get_coherence_per_topic()\n",
    "lemmacoherence_cuci_list = lemmacm_cuci.get_coherence_per_topic()\n",
    "lemmacoherence_cnpmi_list = lemmacm_cnpmi.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence_umass_list:  [-2.9053618393686507, -9.643545524837853, -8.406472330669327, -10.204439489050221, -2.7085719129219723, -2.6264627561477836, -9.073043668616021, -13.77346067592811, -7.6516833860156925, -3.4883574761681877, -5.849989427306162, -9.709326518318544, -8.976672040303493, -6.922241668157372, -7.930321807566442, -6.38605868826715, -8.887889216900867, -5.091815926960557, -2.9668913032543407, -3.526933142002842]\n",
      "coherence_cuci_list:  [-0.8316066708516631, -7.185905439626244, -6.236238756462465, -7.9165808004479725, -0.27173292318072984, -0.9025402426939791, -6.856956439590747, -9.570895778604676, -5.608102330308568, -2.0396706294205518, -3.5421717684064693, -7.982841757140943, -6.280550124105568, -4.7412603813549365, -6.278887824722546, -5.060413149294466, -6.801535194714176, -3.267912734892605, -0.5258131660750702, -1.5429179995438058]\n",
      "coherence_cnpmi_list:  [-0.0020064449681975434, -0.2549322078059393, -0.20658208288268237, -0.2752224911850322, 0.036509163207495074, -0.01669033740202375, -0.22588937645177265, -0.32817637829636287, -0.17702577752874746, -0.0531475980563651, -0.09437002931813158, -0.27292208232023685, -0.21484534920838932, -0.16276850744538812, -0.21729674664210274, -0.15832041506899383, -0.2269509796787834, -0.1034344070665895, 0.025410789806615888, -0.04676476584888379]\n"
     ]
    }
   ],
   "source": [
    "print('coherence_umass_list: ', lemmacoherence_umass_list[0:20])\n",
    "print('coherence_cuci_list: ', lemmacoherence_cuci_list[0:20])\n",
    "print('coherence_cnpmi_list: ', lemmacoherence_cnpmi_list[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmacm2 = CoherenceModel(model=lemmamodel, texts=lemmaprocessed_documents, coherence='u_mass')\n",
    "lemmacoherence2 = lemmacm2.get_coherence()\n",
    "lemmacoherence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmacoherence2_list = lemmacm2.get_coherence_per_topic()\n",
    "print(lemmacoherence2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the distance between lemma topics and global topic\n",
    "\n",
    "lemmahellinger_distance = []\n",
    "avg_distance = 0\n",
    "\n",
    "for topic in lemmatopic_distribution:\n",
    "    lemmahellinger_distance.append(hellinger(topic,global_topic))\n",
    "\n",
    "#for (distance,i) in zip(lemmahellinger_distance,range(len(lemmahellinger_distance))):\n",
    "    #print(\"distance between topic\", i , \"and global topic is\", distance)\n",
    "\n",
    "print('average distance', sum(lemmahellinger_distance)/len(lemmahellinger_distance))\n",
    "print(lemmahellinger_distance[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Topic Model - The Heat Map"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plot_difference_plotly(mdiff, title=\"\", annotation=None):\n",
    "    \"\"\"Plot the difference between models.\n",
    "\n",
    "    Uses plotly as the backend.\"\"\"\n",
    "    import plotly.graph_objs as go\n",
    "    import plotly.offline as py\n",
    "\n",
    "    annotation_html = None\n",
    "    if annotation is not None:\n",
    "        annotation_html = [\n",
    "            [\n",
    "                \"+++ {}<br>--- {}\".format(\", \".join(int_tokens), \", \".join(diff_tokens))\n",
    "                for (int_tokens, diff_tokens) in row\n",
    "            ]\n",
    "            for row in annotation\n",
    "        ]\n",
    "\n",
    "    data = go.Heatmap(z=mdiff, colorscale='RdBu', text=annotation_html)\n",
    "    layout = go.Layout(width=950, height=950, title=title, xaxis=dict(title=\"topic\"), yaxis=dict(title=\"topic\"))\n",
    "    py.iplot(dict(data=[data], layout=layout))\n",
    "\n",
    "\n",
    "def plot_difference_matplotlib(mdiff, title=\"\", annotation=None):\n",
    "    \"\"\"Helper function to plot difference between models.\n",
    "\n",
    "    Uses matplotlib as the backend.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    data = ax.imshow(mdiff, cmap='RdBu_r', origin='lower')\n",
    "    plt.title(title)\n",
    "    plt.colorbar(data)\n",
    "\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "    import plotly.offline as py\n",
    "except Exception:\n",
    "    #\n",
    "    # Fall back to matplotlib if we're not in a notebook, or if plotly is\n",
    "    # unavailable for whatever reason.\n",
    "    #\n",
    "    plot_difference = plot_difference_matplotlib\n",
    "else:\n",
    "    py.init_notebook_mode()\n",
    "    plot_difference = plot_difference_plotly"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "num_topics = 5\n",
    "mdiff = np.ones((num_topics, num_topics))\n",
    "np.fill_diagonal(mdiff, 0.)\n",
    "plot_difference(mdiff, title=\"Topic difference (one model) in ideal world\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mdiff, annotation = lemmamodel.diff(model, distance='jaccard', num_words=10)\n",
    "plot_difference(mdiff, title=\"Topic difference (one model) [jaccard distance]\", annotation=annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Dominant topic and its percentage contribution in each document\n",
    "\n",
    "In LDA models, each document is composed of multiple topics. But, typically only one of the topics is dominant. The below code extracts this dominant topic for each sentence and shows the weight of the topic and the keywords in a nicely formatted output."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=preprocessed_documents):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    \n",
    "     # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    \n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=model, corpus=corpus, texts=preprocessed_documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The most representative sentence for each topic\n",
    "\n",
    "Sometimes you want to get samples of sentences that most represent a given topic. This code gets the most exemplar sentence for each topic."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution of Word Counts in Documents\n",
    "\n",
    "When working with a large number of documents, you want to know how big the documents are as a whole and by topic. Let’s plot the document word counts distribution."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "doc_lens = [len(d) for d in df_dominant_topic.Text]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16,7), dpi=160)\n",
    "plt.hist(doc_lens, bins = 1000, color='navy')\n",
    "plt.text(750, 100, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
    "plt.text(750,  90, \"Median : \" + str(round(np.median(doc_lens))))\n",
    "plt.text(750,  80, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
    "plt.text(750,  70, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\n",
    "plt.text(750,  60, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n",
    "\n",
    "plt.gca().set(xlim=(0, 1000), ylabel='Number of Documents', xlabel='Document Word Count')\n",
    "plt.tick_params(size=16)\n",
    "plt.xticks(np.linspace(0,1000,9))\n",
    "plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Counts of Topic Keywords\n",
    "\n",
    "When it comes to the keywords in the topics, the importance (weights) of the keywords matters. Along with that, how frequently the words have appeared in the documents is also interesting to look.\n",
    "\n",
    "Let’s plot the word counts and the weights of each keyword in the same chart.\n",
    "\n",
    "You want to keep an eye out on the words that occur in multiple topics and the ones whose relative frequency is more than the weight. Often such words turn out to be less important. The chart I’ve drawn below is a result of adding several such words to the stop words list in the beginning and re-running the training process."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.font_manager as fm\n",
    "from matplotlib import ft2font\n",
    "from matplotlib.font_manager import ttfFontProperty\n",
    "\n",
    "\n",
    "fpath = 'SaraswatiNormal.ttf'\n",
    "fprop = fm.FontProperties(fname=fpath)\n",
    "\n",
    "font = ft2font.FT2Font(fpath)\n",
    "fprop = fm.FontProperties(fname=fpath)\n",
    "\n",
    "ttfFontProp = ttfFontProperty(font)\n",
    "\n",
    "fontprop = fm.FontProperties(family='sans-serif',\n",
    "                            fname=ttfFontProp.fname,\n",
    "                            size=25,\n",
    "                            stretch=ttfFontProp.stretch,\n",
    "                            style=ttfFontProp.style,\n",
    "                            variant=ttfFontProp.variant,\n",
    "                            weight=ttfFontProp.weight)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.font_manager as fm\n",
    "prop = fm.FontProperties(fname='SaraswatiNormal.ttf')\n",
    "\n",
    "from collections import Counter\n",
    "topics = model.show_topics(num_topics=4,formatted=False)\n",
    "data_flat = [w for w_list in preprocessed_documents for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "#Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE Clustering Chart"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "\n",
    "# n-1 rows each is a vector with i-1 posisitons, where n the number of documents\n",
    "# i the topic number and tmp[i] = probability of topic i\n",
    "num_topic = 10\n",
    "topic_weights = []\n",
    "for row_list in model[corpus]:\n",
    "    tmp = np.zeros(num_topic)\n",
    "    for i, w in row_list:\n",
    "        tmp[i] = w\n",
    "        topic_weights.append(tmp)\n",
    "\n",
    "# Get topic weights\n",
    "# topic_weights = []\n",
    "# for i, row_list in enumerate(model[corpus]):\n",
    "#     topic_weights.append([w for i, w in row_list[0]])\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "# Plot the Topic Clusters using Bokeh\n",
    "output_notebook()\n",
    "n_topics = 4\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "              plot_width=900, plot_height=700)\n",
    "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.ylabel('Hellinger Distance',fontsize=14)\n",
    "plt.xlabel('Topics',fontsize=14)\n",
    "plt.xticks(np.arange(0, 11, 1))\n",
    "plt.yticks(np.arange(0,1,step=0.1))\n",
    "#3plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "#ax.set_yticklabels([])\n",
    "#plt.set_xticklabels([])\n",
    "plt.title('Distance Measurement',fontsize=18)\n",
    "#plt.legend()\n",
    "num_topics = [x for x in range(1,11)]\n",
    "plt.plot(num_topics,hellinger_distance,'k--D',label='Unlemmatized')\n",
    "plt.plot(num_topics,lemmahellinger_distance,'k--s',label='lemmatized')\n",
    "plt.legend(fontsize=14,loc='upper center',ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "labels = [num for num in range(1,11)]\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, hellinger_distance, width, label='Unlemmatized')\n",
    "rects2 = ax.bar(x + width/2, lemmahellinger_distance, width, label='Lemmatized')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_xlabel('Topics',fontsize=14)\n",
    "ax.set_ylabel('Hellinger Distance',fontsize=14)\n",
    "ax.set_title('Hellinger Distance between Topics',fontsize=18)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend(fontsize=14,loc='upper center',ncol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coherence2_list)\n",
    "print(lemmacoherence2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the topic wise coherence \n",
    "import matplotlib.pyplot as plt\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.ylabel('Semantic coherece',fontsize=14)\n",
    "plt.xlabel('Topics',fontsize=14)\n",
    "#3plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "#ax.set_yticklabels([])\n",
    "#plt.set_xticklabels([])\n",
    "plt.title('Topic wise semantic coherence',fontsize=18)\n",
    "#plt.legend()\n",
    "num_topics = [x for x in range(1,11)]\n",
    "plt.plot(num_topics,coherence2_list,'k',label='Unlemma')\n",
    "plt.plot(num_topics,lemmacoherence2_list,'g',label='lemma')\n",
    "plt.legend(loc='upper left',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "labels = [num for num in range(1,11)]\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, coherence2_list, width, label='Unlemmatized')\n",
    "rects2 = ax.bar(x + width/2, lemmacoherence2_list, width, label='Lemmatized')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_xlabel('Topics',fontsize=14)\n",
    "ax.set_ylabel('Semantic coherece',fontsize=14)\n",
    "ax.set_title('Semantic Coherence for Lemma and Unlemma')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend(fontsize=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
